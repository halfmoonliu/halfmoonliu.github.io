<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Word Embeddings on LIU Yun-Chung</title>
    <link>https://halfmoonliu.github.io/tags/word-embeddings/</link>
    <description>Recent content in Word Embeddings on LIU Yun-Chung</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sun, 05 Dec 2021 00:00:00 +0000</lastBuildDate><atom:link href="https://halfmoonliu.github.io/tags/word-embeddings/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Train Word Embeddings with CBOW Model (Link)</title>
      <link>https://halfmoonliu.github.io/posts/train-word-embeddings-with-cbow-model-link/</link>
      <pubDate>Sun, 05 Dec 2021 00:00:00 +0000</pubDate>
      
      <guid>https://halfmoonliu.github.io/posts/train-word-embeddings-with-cbow-model-link/</guid>
      <description>&lt;p&gt;Word embedding is an effective way to represent the relationship between words. Words with similar neighbors (i.e. context) have similar embeddings. In a now well-known paper published by Mikolov et al. (2013), they demonstrated how to use the continuous bag-of-words (CBOW) model to train word embeddings, which can be applied to analogy and other downstream tasks. This project uses NumPy to train word embeddings by using the CBOW model from scratch and applying them to an analogy task.&lt;/p&gt;</description>
    </item>
    
  </channel>
</rss>
