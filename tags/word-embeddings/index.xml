<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Word Embeddings on LIU Yun-Chung</title>
    <link>https://halfmoonliu.github.io/tags/word-embeddings/</link>
    <description>Recent content in Word Embeddings on LIU Yun-Chung</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sun, 05 Dec 2021 00:00:00 +0000</lastBuildDate><atom:link href="https://halfmoonliu.github.io/tags/word-embeddings/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Train Word Embeddings with CBOW Model</title>
      <link>https://halfmoonliu.github.io/posts/train-word-embeddings-with-cbow-model/</link>
      <pubDate>Sun, 05 Dec 2021 00:00:00 +0000</pubDate>
      
      <guid>https://halfmoonliu.github.io/posts/train-word-embeddings-with-cbow-model/</guid>
      <description>&lt;p&gt;Word Embedding is a relatively effective way to represent word meaning. In the influential paper published by Mikolov et colleagues, they demonstrated the method of using continuous bag-of-words (CBOW) model to train word embeddings, which can be applied to analogy and other downstream tasks.This projec uses numpy to train word embeddings using CBOW model from scratch and apply on analogy task.&lt;/p&gt;</description>
    </item>
    
  </channel>
</rss>
