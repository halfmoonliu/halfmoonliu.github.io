<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Word Embeddings on LIU Yun-Chung</title>
    <link>https://halfmoonliu.github.io/tags/word-embeddings/</link>
    <description>Recent content in Word Embeddings on LIU Yun-Chung</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sun, 05 Dec 2021 00:00:00 +0000</lastBuildDate><atom:link href="https://halfmoonliu.github.io/tags/word-embeddings/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Training Word Embeddings from Scratch Using Numpy</title>
      <link>https://halfmoonliu.github.io/posts/training-word-embeddings-from-scratch-using-numpy/</link>
      <pubDate>Sun, 05 Dec 2021 00:00:00 +0000</pubDate>
      
      <guid>https://halfmoonliu.github.io/posts/training-word-embeddings-from-scratch-using-numpy/</guid>
      <description>&lt;p&gt;Word embedding is an effective way to represent words, carrying semantic and syntactic information with a predefined length of numbers. In a well-known paper, Mikolov et al. (2013) demonstrated the Word2vec technique to train word embeddings, including the continuous bag-of-words (CBOW) and skip gram model. Words with similar neighbors (i.e. context) have similar embeddings. The technique has inspired many later works on training word embedding, which can be applied to analogy and other downstream tasks. In this project, I used NumPy to train word embeddings applying the CBOW model to demonstrate the structure and math of the model and applied the resulting embeddings to an analogy task.&lt;/p&gt;</description>
    </item>
    
  </channel>
</rss>
