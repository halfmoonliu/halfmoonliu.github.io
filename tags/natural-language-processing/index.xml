<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Natural Language Processing on LIU Yun-Chung</title>
    <link>https://halfmoonliu.github.io/tags/natural-language-processing/</link>
    <description>Recent content in Natural Language Processing on LIU Yun-Chung</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sun, 05 Dec 2021 00:00:00 +0000</lastBuildDate><atom:link href="https://halfmoonliu.github.io/tags/natural-language-processing/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Train Word Embeddings with CBOW Model</title>
      <link>https://halfmoonliu.github.io/posts/train-word-embeddings-with-cbow-model/</link>
      <pubDate>Sun, 05 Dec 2021 00:00:00 +0000</pubDate>
      
      <guid>https://halfmoonliu.github.io/posts/train-word-embeddings-with-cbow-model/</guid>
      <description>&lt;p&gt;Word embedding is an effective way to represent the relationship between words. Words with similar neighbors (i.e. context) have similar embeddings. In a now well-known paper, Mikolov et al. (2013) demonstrated the Word2vec technique to train word embeddings, including the continuous bag-of-words (CBOW) and skip gram model. The technique has inspired many later works on training word embedding, which can be applied to analogy and other downstream tasks. In this project, I used NumPy to train word embeddings applying the CBOW model to demonstrate the structure and math of the model and applied the resulting embeddings to an analogy task.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Use BiLSTM for Name Entity Recognition</title>
      <link>https://halfmoonliu.github.io/posts/use-bilstm-for-name-entity-recognition/</link>
      <pubDate>Sun, 28 Nov 2021 00:00:00 +0000</pubDate>
      
      <guid>https://halfmoonliu.github.io/posts/use-bilstm-for-name-entity-recognition/</guid>
      <description>&lt;p&gt;Name Entity Recognition (NER) is a natural language processing task for categorizing words into name entities with numerous applications (e.g., searching, text classification, etc.). The following is a demo on how I trained a Bi-directional Long-Short Term Memory (BiLSTM) model to predict name entities with an accuracy of over .95 using the CoNLL-2003 Dataset.&lt;/p&gt;</description>
    </item>
    
  </channel>
</rss>
