<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Natural Language Processing on LIU Yun-Chung</title>
    <link>https://halfmoonliu.github.io/tags/natural-language-processing/</link>
    <description>Recent content in Natural Language Processing on LIU Yun-Chung</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sun, 05 Dec 2021 00:00:00 +0000</lastBuildDate><atom:link href="https://halfmoonliu.github.io/tags/natural-language-processing/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Train Word Embeddings with CBOW Model</title>
      <link>https://halfmoonliu.github.io/posts/train-word-embeddings-with-cbow-model/</link>
      <pubDate>Sun, 05 Dec 2021 00:00:00 +0000</pubDate>
      
      <guid>https://halfmoonliu.github.io/posts/train-word-embeddings-with-cbow-model/</guid>
      <description>&lt;p&gt;Word embedding is an effective way to represent the relationship between words. Words with similar neighbors (i.e. context) have similar embeddings. In a now well-known paper published by Mikolov et al. (2013), they demonstrated how to use the continuous bag-of-words (CBOW) model to train word embeddings, which can be applied to analogy and other downstream tasks. This project uses NumPy to train word embeddings by using the CBOW model from scratch and applying them to an analogy task. &lt;a href=&#34;https://halfmoonliu.github.io/posts/train-word-embeddings-with-cbow-model/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Read more&amp;hellip;&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Use BiLSTM for Name Entity Recognition</title>
      <link>https://halfmoonliu.github.io/posts/use-bilstm-for-name-entity-recognition/</link>
      <pubDate>Sun, 28 Nov 2021 00:00:00 +0000</pubDate>
      
      <guid>https://halfmoonliu.github.io/posts/use-bilstm-for-name-entity-recognition/</guid>
      <description>&lt;p&gt;Name Entity Recognition (NER) is a natural language processing task for categorizing words into name entities with numerous applications (e.g., searching, text classification, etc.). The following is a demo on how I trained a Bi-directional Long-Short Term Memory (BiLSTM) model to predict name entities with an accuracy of over .95 using the CoNLL-2003 Dataset. &lt;a href=&#34;https://halfmoonliu.github.io/posts/train-word-embeddings-with-cbow-model/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Read more&amp;hellip;&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    
  </channel>
</rss>
