<!DOCTYPE html>
<html lang="en-us">
  <head>
    <title>Use BiLSTM for Name Entity Recognition (Link) | LIU Yun-Chung</title>

    <meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">    
<meta name="viewport" content="width=device-width,minimum-scale=1">
<meta name="description" content="Name Entity Recognition (NER) is a natural language processing task to categorize words into name entities, which has many applications (e.g. searching). Here I is a demo of how I trained a bi-Directional Long-Short Term Memory (BiLSTM) model to predict name entities with accuracy of over .95 using CONRLL 2003 dataset.">
<meta name="generator" content="Hugo 0.87.0" />


  <META NAME="ROBOTS" CONTENT="NOINDEX, NOFOLLOW">


<link rel="stylesheet" href="/css/style.css">



<link rel="shortcut icon" href="/images/favicon.ico" type="image/x-icon" />

 
    
<script type="application/javascript">
var doNotTrack = false;
if (!doNotTrack) {
	window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
	ga('create', 'your-google-analytics-id', 'auto');
	
	ga('send', 'pageview');
}
</script>
<script async src='https://www.google-analytics.com/analytics.js'></script>







  </head>

  <body>
    <nav class="navigation">
	
		<a href="/"> <span class="arrow">â†</span>Home</a>
	
	<a href="/posts">Archive</a>
	<a href="/tags">Tags</a>
	<a href="/about">About</a>

	

	
	  <a class="button" href="https://halfmoonliu.github.io/index.xml">Subscribe</a>
	
</nav>


    <main class="main">
      

<section id="single">
    <h1 class="title">Use BiLSTM for Name Entity Recognition (Link)</h1>

    <div class="tip">
        <time datetime="2021-11-28 00:00:00 &#43;0000 UTC">Nov 28, 2021</time>
        <span class="split">
          Â·
        </span>
        <span>
          521 words
        </span>
        <span class="split">
          Â·
        </span>
        <span>
          3 minute read
        </span>
    </div>

    
    
        
  
    <aside class="toc">
      <details>
          <summary>Table of Contents
          </summary>
          <div>
              <nav id="TableOfContents">
  <ul>
    <li><a href="#quick-summary">Quick Summary</a></li>
    <li><a href="#dataset">Dataset:</a></li>
    <li><a href="#step-1-build-corpus--tags-dictionary">Step 1: Build Corpus &amp; Tags dictionary</a></li>
    <li><a href="#step-2-turn-sentences-to-list-of-tokens-and-name-entity-tags">Step 2: Turn sentences to list of tokens and name entity tags</a></li>
    <li><a href="#step-3-map-words-and-tags-to-number">Step 3: Map words and tags to number</a></li>
    <li><a href="#step-4-padding">Step 4: Padding</a></li>
    <li><a href="#step-5-model-buildup">Step 5: Model Buildup</a></li>
    <li><a href="#step-6-model-training">Step 6: Model training</a></li>
    <li><a href="#step-7-validation">Step 7: Validation</a></li>
    <li><a href="#step-8-test-the-model-with-unseen-test-data">Step 8: Test the Model with Unseen Test Data</a></li>
  </ul>
</nav>
          </div>
      </details>
    </aside>
  


    


    <div class="content">
      <p>Name Entity Recognition (NER) is a natural language processing task to categorize words into name entities, which has many applications (e.g. searching). Here I is a demo of how I trained a bi-Directional Long-Short Term Memory (BiLSTM) model to predict name entities with accuracy of over .95 using CONRLL 2003 dataset.</p>
<h2 id="quick-summary">Quick Summary <a href="#quick-summary" class="anchor">ğŸ”—</a></h2><p>This <a href="https://github.com/halfmoonliu/NER" target="_blank" rel="noopener">project</a> demonstrates the workflow of building recurrent neural network model to predict name entity labels using TensorFlow. With a simple structure of one embedding layer, one bidirectional LSTM layer and one dense layer, prediction accuracy on held-out test data set can achieve above .95 on CONLL-2003 data set. a Below is a brief introduction of the workflow (tokenization, padding, model buildup, hyperparameter tuning and test).</p>
<h2 id="dataset">Dataset: <a href="#dataset" class="anchor">ğŸ”—</a></h2><p>The CONLL-2003 dataset were already separated into train, validation and test sets. All three datasets contain sentences and name-entity tags corresponding to each word in the sentences.</p>
<h2 id="step-1-build-corpus--tags-dictionary">Step 1: Build Corpus &amp; Tags dictionary <a href="#step-1-build-corpus--tags-dictionary" class="anchor">ğŸ”—</a></h2><p>To convert words to numbers, we first read in the sentences and name entities in the training dataset, break sentences into lists of words (tokenization) and tags and create a dictionary &ldquo;Corpus&rdquo; to map every word to a number, plus an &ldquo;UNK&rdquo; token for unknown words and another dictionary  &ldquo;Tags&rdquo; to store mapping of all name entity tags and the corresponding number.</p>
<h2 id="step-2-turn-sentences-to-list-of-tokens-and-name-entity-tags">Step 2: Turn sentences to list of tokens and name entity tags <a href="#step-2-turn-sentences-to-list-of-tokens-and-name-entity-tags" class="anchor">ğŸ”—</a></h2><p>To convert words and name entity tags into numbers, We need to first break sentences in to lists of words (or tokens).</p>
<h2 id="step-3-map-words-and-tags-to-number">Step 3: Map words and tags to number <a href="#step-3-map-words-and-tags-to-number" class="anchor">ğŸ”—</a></h2><p>With a map of token-number pair and tag-number pairs, we can convert the sentences in the dataset into numbers for model build-up later.</p>
<h2 id="step-4-padding">Step 4: Padding <a href="#step-4-padding" class="anchor">ğŸ”—</a></h2><p>To make all sentences in the dataset the same length to put build models, we add zeros to the end of each sentence (called padding) to make them all the same length of the longest sentence.</p>
<h2 id="step-5-model-buildup">Step 5: Model Buildup <a href="#step-5-model-buildup" class="anchor">ğŸ”—</a></h2><p>I used TensorFlow to build a sequence model containing an embedding layer, a bi-directional Long-Short Term Memory (LSTM) layer and a dense layer. The structures (layers in the model) and other hyperparameters (e.g. learning rate, activation function, dropout rate, regularization) are all hyper parameters which can be tunned to optimize evaluation matrices using the training and validation data set.</p>
<h2 id="step-6-model-training">Step 6: Model training <a href="#step-6-model-training" class="anchor">ğŸ”—</a></h2><p>After defining model structure, we can feed the preprocessed (tokenized, mapped and padded) training data to train the model. Epochs (times the model runs through all the training data) and batch size (during an epoch, training data are processed on batch at a time until the whole training set is processed) are to be adjusted to maximize performance.</p>
<h2 id="step-7-validation">Step 7: Validation <a href="#step-7-validation" class="anchor">ğŸ”—</a></h2><p>Hyperparameters can be tuned to maximize the model performance on the validation set. The performance on the training and validation set both serve as reference for hyperparameter tuning(e.g. bias-variance issue).</p>
<h2 id="step-8-test-the-model-with-unseen-test-data">Step 8: Test the Model with Unseen Test Data <a href="#step-8-test-the-model-with-unseen-test-data" class="anchor">ğŸ”—</a></h2><p>The final model and hyperparameters should be tested on unseen (held-out) dataset. This is to avoid overfitting due to hyperparameter tuning. The testing accuracy reached above .95 using bidirectional LSTM on CONLL-2003 test data.</p>
    </div>

    
        <div class="tags">
            
                <a href="https://halfmoonliu.github.io/tags/machine-learning">Machine Learning</a>
            
                <a href="https://halfmoonliu.github.io/tags/deep-learning">Deep Learning</a>
            
                <a href="https://halfmoonliu.github.io/tags/natural-language-processing">Natural Language Processing</a>
            
        </div>
    
    
    
  <div id="comment">
    
    <div id="disqus_thread"></div>
<script type="application/javascript">
    var disqus_config = function () {
    
    
    
    };
    (function() {
        if (["localhost", "127.0.0.1"].indexOf(window.location.hostname) != -1) {
            document.getElementById('disqus_thread').innerHTML = 'Disqus comments not available by default when the website is previewed locally.';
            return;
        }
        var d = document, s = d.createElement('script'); s.async = true;
        s.src = '//' + "your-disqus-shortname" + '.disqus.com/embed.js';
        s.setAttribute('data-timestamp', +new Date());
        (d.head || d.body).appendChild(s);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="https://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
  </div>


</section>


    </main>
    
    <footer id="footer">
    
        <div id="social">


    <a class="symbol" href="https://github.com/halfmoonliu" rel="me" target="_blank">
        
        <svg fill="#bbbbbb" width="28" height="28"  viewBox="0 0 72 72" version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink">
    
    <title>Github</title>
    <desc>Created with Sketch.</desc>
    <defs></defs>
    <g id="Page-1" stroke="none" stroke-width="1" fill="none" fill-rule="evenodd">
        <g id="Social-Icons---Rounded-Black" transform="translate(-264.000000, -939.000000)">
            <g id="Github" transform="translate(264.000000, 939.000000)">
                <path d="M8,72 L64,72 C68.418278,72 72,68.418278 72,64 L72,8 C72,3.581722 68.418278,-8.11624501e-16 64,0 L8,0 C3.581722,8.11624501e-16 -5.41083001e-16,3.581722 0,8 L0,64 C5.41083001e-16,68.418278 3.581722,72 8,72 Z" id="Rounded" fill="#bbbbbb"></path>
                <path d="M35.9985,13 C22.746,13 12,23.7870921 12,37.096644 C12,47.7406712 18.876,56.7718301 28.4145,59.9584121 C29.6145,60.1797862 30.0525,59.4358488 30.0525,58.7973276 C30.0525,58.2250681 30.0315,56.7100863 30.0195,54.6996482 C23.343,56.1558981 21.9345,51.4693938 21.9345,51.4693938 C20.844,48.6864054 19.2705,47.9454799 19.2705,47.9454799 C17.091,46.4500754 19.4355,46.4801943 19.4355,46.4801943 C21.843,46.6503662 23.1105,48.9634994 23.1105,48.9634994 C25.2525,52.6455377 28.728,51.5823398 30.096,50.9649018 C30.3135,49.4077535 30.9345,48.3460615 31.62,47.7436831 C26.2905,47.1352808 20.688,45.0691228 20.688,35.8361671 C20.688,33.2052792 21.6225,31.0547881 23.1585,29.3696344 C22.911,28.7597262 22.0875,26.3110578 23.3925,22.9934585 C23.3925,22.9934585 25.4085,22.3459017 29.9925,25.4632101 C31.908,24.9285993 33.96,24.6620468 36.0015,24.6515052 C38.04,24.6620468 40.0935,24.9285993 42.0105,25.4632101 C46.5915,22.3459017 48.603,22.9934585 48.603,22.9934585 C49.9125,26.3110578 49.089,28.7597262 48.8415,29.3696344 C50.3805,31.0547881 51.309,33.2052792 51.309,35.8361671 C51.309,45.0917119 45.6975,47.1292571 40.3515,47.7256117 C41.2125,48.4695491 41.9805,49.9393525 41.9805,52.1877301 C41.9805,55.4089489 41.9505,58.0067059 41.9505,58.7973276 C41.9505,59.4418726 42.3825,60.1918338 43.6005,59.9554002 C53.13,56.7627944 60,47.7376593 60,37.096644 C60,23.7870921 49.254,13 35.9985,13" fill="#FFFFFF"></path>
            </g>
        </g>
    </g>
</svg>
    </a>


</div>

    

    <div class="copyright">
    
       Â© Copyright 
       2022 
       <span class="split">
        <svg fill="#bbbbbb" width="15" height="15" version="1.1" id="heart-15" xmlns="http://www.w3.org/2000/svg" width="15px" height="15px" viewBox="0 0 15 15">
  <path d="M13.91,6.75c-1.17,2.25-4.3,5.31-6.07,6.94c-0.1903,0.1718-0.4797,0.1718-0.67,0C5.39,12.06,2.26,9,1.09,6.75&#xA;&#x9;C-1.48,1.8,5-1.5,7.5,3.45C10-1.5,16.48,1.8,13.91,6.75z"/>
</svg>
       </span>
       LIU Yun-Chung
    
    </div>

    
      <div class="powerby">
        Powered by <a href='http://www.gohugo.io/'>Hugo</a> Theme By <a href='https://github.com/nodejh/hugo-theme-cactus-plus'>nodejh</a>
      </div>
    
</footer>



  </body>
</html>
