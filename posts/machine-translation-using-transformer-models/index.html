<!DOCTYPE html>
<html lang="en-us">
  <head>
    <title>Machine Translation Using Transformer Models | LIU Yun-Chung</title>

    <meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">    
<meta name="viewport" content="width=device-width,minimum-scale=1">
<meta name="description" content="One of recent breakthroughs in machine translation is the application of the transformer. Compared with the sequence-to-sequence model based on recurrent neural networks (RNN), the attention-based transformer model makes use of encoder output at every time step, which further enhances machine translation performances. In this post, I will walk through the structure of RNN- and transformer-based and machine translation models, steps to build them and compare their performances.">
<meta name="generator" content="Hugo 0.87.0" />


  <META NAME="ROBOTS" CONTENT="NOINDEX, NOFOLLOW">


<link rel="stylesheet" href="/css/style.css">



<link rel="shortcut icon" href="/images/favicon.ico" type="image/x-icon" />

 
    
<script type="application/javascript">
var doNotTrack = false;
if (!doNotTrack) {
	window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
	ga('create', 'your-google-analytics-id', 'auto');
	
	ga('send', 'pageview');
}
</script>
<script async src='https://www.google-analytics.com/analytics.js'></script>







  </head>

  <body>
    <nav class="navigation">
	
		<a href="/"> <span class="arrow">‚Üê</span>Home</a>
	
	<a href="/posts">Archive</a>
	<a href="/tags">Tags</a>
	<a href="/about">About</a>

	

	
	  <a class="button" href="https://halfmoonliu.github.io/index.xml">Subscribe</a>
	
</nav>


    <main class="main">
      

<section id="single">
    <h1 class="title">Machine Translation Using Transformer Models</h1>

    <div class="tip">
        <time datetime="2022-04-06 00:00:00 &#43;0000 UTC">Apr 6, 2022</time>
        <span class="split">
          ¬∑
        </span>
        <span>
          1006 words
        </span>
        <span class="split">
          ¬∑
        </span>
        <span>
          5 minute read
        </span>
    </div>

    
    
        
  
    <aside class="toc">
      <details>
          <summary>Table of Contents
          </summary>
          <div>
              <nav id="TableOfContents">
  <ul>
    <li><a href="#background">Background</a></li>
    <li><a href="#dataset">Dataset</a></li>
    <li><a href="#preprocessing">Preprocessing</a></li>
    <li><a href="#model-setup">Model Setup</a></li>
    <li><a href="#model-output">Model Output</a>
      <ul>
        <li><a href="#example-output-seqtoseq-model-without-transformer">Example Output: SeqToSeq model without transformer</a></li>
        <li><a href="#example-output-seqtoseq-model-with-transformer">Example Output: SeqToSeq model with transformer</a></li>
      </ul>
    </li>
    <li><a href="#measure-model-performance-the-bleu-score">Measure Model Performance: the BLEU Score</a></li>
    <li><a href="#result-and-next-steps">Result and Next Steps</a></li>
    <li><a href="#reference">Reference</a></li>
  </ul>
</nav>
          </div>
      </details>
    </aside>
  


    


    <div class="content">
      <p>One of recent breakthroughs in machine translation is the application of the transformer. Compared with the sequence-to-sequence model based on recurrent neural networks (RNN), the attention-based transformer model makes use of encoder output at every time step, which further enhances machine translation performances. In this post, I will walk through the structure of RNN- and transformer-based and machine translation models, steps to build them and compare their performances.</p>
<h2 id="background">Background <a href="#background" class="anchor">üîó</a></h2><p>Recently, machine translation has gained much success due to a series of endeavors to represent sentences using an sequence-to-sequence (seq-to-seq) model (Please click here for <a href="https://github.com/halfmoonliu/SeqToSeq" target="_blank" rel="noopener">my demo of the seq-to-seq based machine translation model</a>). In this structure, two sequence model are put together: the first one, the encoder, encodes the information of the source sentence (sentence to be translated when the structure is applied in machine translation); the second one, the decoder, produces the sentence in target language word by word based on the encoding (embedding) of the source sentence. This avoids the problem of word by word translation since languages vary in their syntax. For example, the French sentence &ldquo;Je ne l‚Äôaime pas‚Äù can be translated into English as ‚ÄúI don‚Äôt like him‚Äù. ‚ÄúNe‚Ä¶pas‚Äù in French is equivalent to ‚Äúdo not&rdquo; in English. Due to the word order difference in the two languages, word-by-word translation is difficult. The encoder-decoder structure addressed this issue and gained better performance on machine translation tasks.</p>
<h2 id="dataset">Dataset <a href="#dataset" class="anchor">üîó</a></h2><p>The source of the dataset used in this project is <a href="https://tatoeba.org/en" target="_blank" rel="noopener">Tatoeba</a>, a website where people can upload sentences in any language and contribute their own versions of translations in other languages. The English-French dataset used here was downloaded from <a href="https://www.manythings.org/anki/," target="_blank" rel="noopener">https://www.manythings.org/anki/,</a> where they preprocessed the Tatoeba dataset so that it became a text file of English French sentence pairs.</p>
<h2 id="preprocessing">Preprocessing <a href="#preprocessing" class="anchor">üîó</a></h2><p>The dataset is ingested and the following steps are done before building the machine translation model:</p>
<ol>
<li>Texts are transformed to lower case letters, punctuations are removed.</li>
<li>Word-number dictionaries are created to map every word appeared in the dataset to a unique number before putting in the model.</li>
</ol>
<h2 id="model-setup">Model Setup <a href="#model-setup" class="anchor">üîó</a></h2><ol>
<li>
<p>Encoder: The first sequence model in the seq-to-seq structure, the encoder, encodes the information of the sentence in the source language. The encoder outputs a fixed length embedding, which will be the input of the second sequence model, the decoder, to output prediction of word in target language. Several types of layers in the sequence model family can be used as the encoder: for example, RNN (recurrent neural network), GRU (gated  recurrent unit),  LSTM (long-short term memory) are all candidates.</p>
</li>
<li>
<p>Decoder: The second sequence model in the seq-to-seq structure, the decoder, takes the weights in the hidden layer of the encoder as input and generates sentences in target language in a word-by-word fashion.</p>
</li>
<li>
<p>Decoder with attention: Instead of taking the last output of the coder, a decoder with attention multiplies output at each step with all encoder outputs to and learns to ‚Äòfocus attention‚Äô on different output, which leads to better performance.</p>
</li>
</ol>
<h2 id="model-output">Model Output <a href="#model-output" class="anchor">üîó</a></h2><p>After training for 60,000 iterations, we show some examples of model generated sentences in the target language using the test dataset unseen by the model.</p>
<h3 id="example-output-seqtoseq-model-without-transformer">Example Output: SeqToSeq model without transformer <a href="#example-output-seqtoseq-model-without-transformer" class="anchor">üîó</a></h3><p><!-- raw HTML omitted -->Almost Successful Example<!-- raw HTML omitted -->:</p>
<p>(Eng) thanks for your hard work. - (French) merci d avoir travaille si durement.</p>
<p>Model generated translation from Eng:  merci pour ton travail.</p>
<p><!-- raw HTML omitted -->Failed Example<!-- raw HTML omitted -->:</p>
<p>(Eng) we re not family. - (French) nous ne sommes pas de la meme famille.</p>
<p>Model generated translation from Eng:  nous ne sommes pas.</p>
<h3 id="example-output-seqtoseq-model-with-transformer">Example Output: SeqToSeq model with transformer <a href="#example-output-seqtoseq-model-with-transformer" class="anchor">üîó</a></h3><p><!-- raw HTML omitted -->Almost Successful Example<!-- raw HTML omitted -->:</p>
<p>(Eng) don t leave them alone. - (French) ne les laissez pas seuls.</p>
<p>Model generated translation from Eng:  ne te laisse pas seule!</p>
<p><!-- raw HTML omitted -->Failed Example<!-- raw HTML omitted -->:</p>
<p>(Eng) you re the best singer i know. - (French) vous etes le meilleur chanteur que je connaisse .</p>
<p>Model generated translation from Eng:  vous etes la meilleure . . .</p>
<h2 id="measure-model-performance-the-bleu-score">Measure Model Performance: the BLEU Score <a href="#measure-model-performance-the-bleu-score" class="anchor">üîó</a></h2><p>It is difficult to compare model performance of machine translation by random sampling model outputs. One possible index to measure translation performance is to use the BLEU score.</p>
<p>The BLEU score, simply put, is the number of tokens generated by the model matching the correct answer generated by humans, divided by the total number of tokens in the correct answer, ignoring word orders. The closer the BLEU score is to 1, the better the generated translation is. Consider the following ground truth sentence pair:</p>
<p>(Eng) How are you doing - (French) Comment ca va</p>
<p>Based on the English sentence, the model generates the following sentence:</p>
<p>Tout va bien</p>
<p>If we measure the number 1 gram in the ground truth appears in the generated sentence, there is only va, which leads to the BLEU score of 1 / 3 ~ 0.33.</p>
<p>If we measure the number 2-grams in the ground truth that appear in the generated sentence, there is zero (Comment ca &amp; ca va), which leads to the BLEU score of 0 / 2 = 0.</p>
<p>From this example, we can see the problem of BLEU score. The number of overlapping n-grams is not a perfect measure for translation because &lsquo;comment ca va&rsquo; and ' and &lsquo;tout va bien&rsquo; can both be correct translations for &lsquo;how are you doing&rsquo;. However, the BLEU score is still a widely used measure for machine learning tasks using large amounts of data.</p>
<h2 id="result-and-next-steps">Result and Next Steps <a href="#result-and-next-steps" class="anchor">üîó</a></h2><p>The 1-gram BLEU score of the Seq-to-Seq with transformer model on the test data set is .47, higher than the model without transformer (.44) in the demonstrated experiment.</p>
<p>Next Step: model enhancement.</p>
<h2 id="reference">Reference <a href="#reference" class="anchor">üîó</a></h2><ol>
<li>Cho, K., van Merrienboer, B., G√ºl√ßehre, √á., Bougares, F., Schwenk, H., and Bengio,
Y (2014). Learning phrase representations using RNN encoder-decoder for statistical machine translation. In Conference on Empirical Methods in Natural Language Processing.</li>
<li>Sutskever, I., Vinyals, O., and Le, Q. (2014). Sequence to sequence learning with neural networks. In Advances in Neural Information Processing Systems (NIPS 2014).</li>
<li>Papineni, K., Roukos, S., Ward, T., and Zhu. W. J. (2002). BLEU: a method for automatic evaluation of machine translation. In ACL, 2002.</li>
</ol>
    </div>

    
        <div class="tags">
            
                <a href="https://halfmoonliu.github.io/tags/transformer">Transformer</a>
            
                <a href="https://halfmoonliu.github.io/tags/recurrent-neural-network">Recurrent Neural Network</a>
            
                <a href="https://halfmoonliu.github.io/tags/machine-translation">Machine Translation</a>
            
        </div>
    
    
    
  <div id="comment">
    
    <div id="disqus_thread"></div>
<script type="application/javascript">
    var disqus_config = function () {
    
    
    
    };
    (function() {
        if (["localhost", "127.0.0.1"].indexOf(window.location.hostname) != -1) {
            document.getElementById('disqus_thread').innerHTML = 'Disqus comments not available by default when the website is previewed locally.';
            return;
        }
        var d = document, s = d.createElement('script'); s.async = true;
        s.src = '//' + "your-disqus-shortname" + '.disqus.com/embed.js';
        s.setAttribute('data-timestamp', +new Date());
        (d.head || d.body).appendChild(s);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="https://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
  </div>


</section>


    </main>
    
    <footer id="footer">
    
        <div id="social">


    <a class="symbol" href="https://github.com/halfmoonliu" rel="me" target="_blank">
        
        <svg fill="#bbbbbb" width="28" height="28"  viewBox="0 0 72 72" version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink">
    
    <title>Github</title>
    <desc>Created with Sketch.</desc>
    <defs></defs>
    <g id="Page-1" stroke="none" stroke-width="1" fill="none" fill-rule="evenodd">
        <g id="Social-Icons---Rounded-Black" transform="translate(-264.000000, -939.000000)">
            <g id="Github" transform="translate(264.000000, 939.000000)">
                <path d="M8,72 L64,72 C68.418278,72 72,68.418278 72,64 L72,8 C72,3.581722 68.418278,-8.11624501e-16 64,0 L8,0 C3.581722,8.11624501e-16 -5.41083001e-16,3.581722 0,8 L0,64 C5.41083001e-16,68.418278 3.581722,72 8,72 Z" id="Rounded" fill="#bbbbbb"></path>
                <path d="M35.9985,13 C22.746,13 12,23.7870921 12,37.096644 C12,47.7406712 18.876,56.7718301 28.4145,59.9584121 C29.6145,60.1797862 30.0525,59.4358488 30.0525,58.7973276 C30.0525,58.2250681 30.0315,56.7100863 30.0195,54.6996482 C23.343,56.1558981 21.9345,51.4693938 21.9345,51.4693938 C20.844,48.6864054 19.2705,47.9454799 19.2705,47.9454799 C17.091,46.4500754 19.4355,46.4801943 19.4355,46.4801943 C21.843,46.6503662 23.1105,48.9634994 23.1105,48.9634994 C25.2525,52.6455377 28.728,51.5823398 30.096,50.9649018 C30.3135,49.4077535 30.9345,48.3460615 31.62,47.7436831 C26.2905,47.1352808 20.688,45.0691228 20.688,35.8361671 C20.688,33.2052792 21.6225,31.0547881 23.1585,29.3696344 C22.911,28.7597262 22.0875,26.3110578 23.3925,22.9934585 C23.3925,22.9934585 25.4085,22.3459017 29.9925,25.4632101 C31.908,24.9285993 33.96,24.6620468 36.0015,24.6515052 C38.04,24.6620468 40.0935,24.9285993 42.0105,25.4632101 C46.5915,22.3459017 48.603,22.9934585 48.603,22.9934585 C49.9125,26.3110578 49.089,28.7597262 48.8415,29.3696344 C50.3805,31.0547881 51.309,33.2052792 51.309,35.8361671 C51.309,45.0917119 45.6975,47.1292571 40.3515,47.7256117 C41.2125,48.4695491 41.9805,49.9393525 41.9805,52.1877301 C41.9805,55.4089489 41.9505,58.0067059 41.9505,58.7973276 C41.9505,59.4418726 42.3825,60.1918338 43.6005,59.9554002 C53.13,56.7627944 60,47.7376593 60,37.096644 C60,23.7870921 49.254,13 35.9985,13" fill="#FFFFFF"></path>
            </g>
        </g>
    </g>
</svg>
    </a>


</div>

    

    <div class="copyright">
    
       ¬© Copyright 
       2023 
       <span class="split">
        <svg fill="#bbbbbb" width="15" height="15" version="1.1" id="heart-15" xmlns="http://www.w3.org/2000/svg" width="15px" height="15px" viewBox="0 0 15 15">
  <path d="M13.91,6.75c-1.17,2.25-4.3,5.31-6.07,6.94c-0.1903,0.1718-0.4797,0.1718-0.67,0C5.39,12.06,2.26,9,1.09,6.75&#xA;&#x9;C-1.48,1.8,5-1.5,7.5,3.45C10-1.5,16.48,1.8,13.91,6.75z"/>
</svg>
       </span>
       LIU Yun-Chung
    
    </div>

    
</footer>



  </body>
</html>
